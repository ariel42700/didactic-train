---
title: "Sentiment Analysis"
output: html_document
date: "2023-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sentiment Analysis on Tweets about Ronaldo

```{r}
# List of packages to install
packages_to_install <- c("hms", "lubridate", "tidytext", "tm", "wordcloud",
                         "igraph", "glue", "networkD3", "plyr", "stringr",
                         "ggplot2", "ggeasy", "plotly", "dplyr", "hms",
                         "lubridate", "magrittr", "tidyverse", "janeaustenr",
                         "widyr")

# Install packages
#chooseCRANmirror(graphics=FALSE)
#install.packages(packages_to_install)
library(hms)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)

file_path <- "data/Cleaned_ronaldo_tweets.csv"

tweets_df <- read.csv(file_path)

str(tweets_df)

# load sentiment 
positive = scan('data/resources/positive-words.txt', what = 'character', comment.char = ';')

negative = scan('data/resources/negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'upgrade','Congrats','prizes','prize','thanks','thnx',
              'Grt','gr8','plz','trending','recovering','brainstorm','leader')
neg.words = c(negative,'wtf','wait','waiting','epicfail','Fight','fighting',
              'arrest','no','not')


```

```{r Data Cleaning}

```


```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none') {
  require(plyr)
  require(stringr)
  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

chunk_size <- 100 # Adjust the size based on your preference

# Get the number of chunks
num_chunks <- ceiling(nrow(tweets_df) / chunk_size)

print(num_chunks)

```

```{r}
cleanText <- tweets_df$content
analysis <- score.sentiment(cleanText, pos.words, neg.words)
table(analysis$score)


```

```{r}
# plot of sentiment frequencies
analysis %>%
  ggplot(aes(x=score)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(analysis$score), sd = sd(analysis$score)), color = "orange", size = 1) +  # Add smoothed curve
  ylab("Frequency/Density") +
  xlab("Sentiment Score") +
  ggtitle("Distribution of Sentiment Scores of the Tweets") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Add vertical line at score 0
  ggeasy::easy_center_title()
  
```


## Bootstrapping Sentiment Analysis
```{r bootstrap}
# Bootstrap with 100 iterations
bootstrap_results <- replicate(100, analysis$score)

table(bootstrap_results)

# creating df to graphically visualize distribution of results
bootstrap_df <- data.frame(Sentiment_Score = c(bootstrap_results))
  
ggplot(bootstrap_df, aes(x = Sentiment_Score)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sentiment Scores", x = "Sentiment Score", y = "Frequency")
```
