---
title: "Sentiment Analysis"
output: html_document
date: "2023-12-01"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sentiment Analysis on Tweets about Ronaldo

```{r}
# List of packages to install
packages_to_install <- c("hms", "lubridate", "tidytext", "tm", "wordcloud",
                         "igraph", "glue", "networkD3", "plyr", "stringr",
                         "ggplot2", "ggeasy", "plotly", "dplyr", "hms",
                         "lubridate", "magrittr", "tidyverse", "janeaustenr",
                         "widyr")

# Install packages
#chooseCRANmirror(graphics=FALSE)
#install.packages(packages_to_install)
library(hms)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(plyr)
library(stringr)
library(stringi)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)


file_path <- "data/Cleaned_ronaldo_tweets.csv"

tweets_df <- read.csv(file_path)

str(tweets_df)

# load sentiment 
positive = scan('data/resources/positive-words.txt', what = 'character', comment.char = ';')
positive_slang_positive <- scan('data/resources/positive-soccer-words.txt', what = 'character', comment.char = ';') 
negative = scan('data/resources/negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
# Positive slang words in soccer
# Negative soccer slang words vector
soccer_slang_negative <- c("flopper", "diver", "choker", "lazy", "flop", "wasteful",
                            "slump", "hoofball", "showboating", "hack", "stagnant",
                            "sloppy", "lackluster", "soft", "bottler", "whiner",
                            "inconsistent", "flat-footed", "stiff", "disjointed", "jaded", "shafted", "shite")

# Print the vector
print(soccer_slang_negative)

# Ensure soccer_slang_positive words are not already in pos.words
new_positive_words <- soccer_slang_positive[!soccer_slang_positive %in% positive]
new_negative_words <- soccer_slang_positive[!soccer_slang_negative %in% negative]

pos.words = c(positive, new_positive_words,'upgrade','Congrats','prizes','prize','thanks','thnx',
              'grt','gr8','plz','trending','recovering','brainstorm','leader')

neg.words = c(negative, new_negative_words, 'wtf','wait','waiting','epicfail','fight','fighting',
              'arrest','no','not', "mfker", "mf")
```

```{r}
# Convert 'date' variable to DateTime format
tweets_df$date <- ymd_hms(tweets_df$date)

# Generate a sequence of all days within the range of your data
all_days <- seq(min(tweets_df$date), max(tweets_df$date), by = "day")

# Identify missing days
missing_days <- setdiff(all_days, tweets_df$date)

# Print missing days
if (length(missing_days) > 0) {
  cat("Missing days:\n")
  print(missing_days)
} else {
  cat("No missing days found.\n")
}
```
```{r}
# Example conversion of Unix timestamps to human-readable dates
readable_dates <- as.POSIXct(missing_days)

# Print the converted dates
print(readable_dates)

```

```{r}
# Assuming tweets_df$date is already in POSIXct format

# Find the unique dates in the dataset
unique_dates <- unique(as.Date(tweets_df$date))

# Initialize a list to store subsets
consecutive_data_subsets <- list()

# Loop through the unique dates
for (i in seq_along(unique_dates)) {
  # Find the consecutive dates until a gap is detected
  consecutive_dates <- unique_dates[i]
  
  while ((consecutive_dates[length(consecutive_dates)] + 1) %in% unique_dates) {
    consecutive_dates <- c(consecutive_dates, consecutive_dates[length(consecutive_dates)] + 1)
  }
  
  # Identify the indices corresponding to the consecutive dates
  indices <- which(as.Date(tweets_df$date) %in% consecutive_dates)
  
  # Add the subset to the list
  consecutive_data_subsets[[i]] <- tweets_df[indices, ]
}

# Print the subsets
for (i in seq_along(consecutive_data_subsets)) {
  cat("Subset", i, ":\n")
  print(consecutive_data_subsets[[i]])
  cat("\n")
}

```
```{r}
# Assuming consecutive_data_subsets is a list of subsets

# Print the range of dates for each subset in a human-readable format
for (i in seq_along(consecutive_data_subsets)) {
  subset_range <- range(consecutive_data_subsets[[i]]$date)
  formatted_range <- format(subset_range, "%Y-%m-%d %H:%M:%S")
  cat("Subset", i, "Date Range: ", formatted_range[1], " to ", formatted_range[2], "\n")
}

```







```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none') {
  require(plyr)
  require(stringr)
  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # convert to lower case:
    sentence = tolower(sentence)
    
    # use regex to extract words
    words = str_extract_all(sentence, "\\b\\w+\\b") %>% unlist()
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = words %in% pos.words
    neg.matches = words %in% neg.words
    
    # Create a logical vector indicating positive words preceded by negative words
    decrease_score_mask <- pos.matches & c(FALSE, head(neg.matches, -1))
    
    score <- sum(pos.matches) - sum(neg.matches) - sum(decrease_score_mask)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

chunk_size <- 100 # Adjust the size based on your preference

# Get the number of chunks
num_chunks <- ceiling(nrow(tweets_df) / chunk_size)

print(num_chunks)

```

```{r}
library(plotly)
cleanText <- tweets_df$content
analysis <- score.sentiment(cleanText, pos.words, neg.words)
table(analysis$score)


```
```{r}
library(purrr)

# Define a function to perform sentiment analysis on a subset
perform_sentiment_analysis <- function(text, pos.words, neg.words) {
  score.sentiment(text, pos.words, neg.words)$score
}

# Use purrr::map to apply the sentiment analysis function to each subset
subset_sentiments <- map(consecutive_data_subsets, ~perform_sentiment_analysis(.x$content, pos.words, neg.words))
```

```{r}
library(ggplot2)

# Assuming subset_sentiments is a list of numeric vectors

# Plot each subset's density curve for all scores
subset_density_plots <- lapply(seq_along(subset_sentiments), function(i) {
  subset_mean <- mean(subset_sentiments[[i]])
  
  ggplot(data.frame(Sentiment_Score = subset_sentiments[[i]]), aes(x = Sentiment_Score)) +
    geom_density(fill = i, alpha = 0.5) +
    geom_vline(xintercept = subset_mean, linetype = "dashed", color = "red") +
    geom_text(aes(x = subset_mean, label = sprintf("Mean: %.2f", subset_mean)),
              y = 0, vjust = -1, color = "red", size = 3) +  # Add label for mean
    labs(title = paste("Subset", i, "Sentiment Score Density"), x = "Sentiment Score", y = "Density") +
    theme_minimal()
})

# Plot the overall density curve for all scores
overall_scores <- unlist(subset_sentiments)
overall_mean <- mean(overall_scores)
overall_density_plot <- ggplot(data.frame(Sentiment_Score = overall_scores), aes(x = Sentiment_Score)) +
  geom_density(fill = "blue", alpha = 0.5) +
  geom_vline(xintercept = overall_mean, linetype = "dashed", color = "red") +
  geom_text(aes(x = overall_mean, label = sprintf("Mean: %.2f", overall_mean)),
            y = 0, vjust = -1, color = "red", size = 3) +  # Add label for mean
  labs(title = "Overall Sentiment Score Density", x = "Sentiment Score", y = "Density") +
  theme_minimal()

# Print each subset's density plot along with the overall plot
for (i in seq_along(subset_density_plots)) {
  print(subset_density_plots[[i]])
}

print(overall_density_plot)

```

```{r}
library(ggplot2)

# Assuming subset_sentiments is a list of numeric vectors

# Calculate mean sentiments for each subset
subset_means <- sapply(subset_sentiments, mean)

# Find the index of the subset with the highest mean sentiment
subset_highest_mean_index <- which.max(subset_means)

# Find the index of the subset with the lowest mean sentiment
subset_lowest_mean_index <- which.min(subset_means)

# Plot the subset with the highest mean sentiment
subset_highest_mean_plot <- ggplot(data.frame(Sentiment_Score = subset_sentiments[[subset_highest_mean_index]]), 
                                   aes(x = Sentiment_Score)) +
  geom_density(fill = subset_highest_mean_index, alpha = 0.5) +
  geom_vline(xintercept = subset_means[subset_highest_mean_index], linetype = "dashed", color = "red") +
  geom_text(aes(x = subset_means[subset_highest_mean_index], 
                label = sprintf("Mean: %.2f", subset_means[subset_highest_mean_index])),
            y = 0, vjust = -1, color = "red", size = 3) +  # Add label for mean
  labs(title = paste("Subset", subset_highest_mean_index, "Sentiment Score Density"), 
       x = "Sentiment Score", y = "Density") +
  theme_minimal()

# Plot the subset with the lowest mean sentiment
subset_lowest_mean_plot <- ggplot(data.frame(Sentiment_Score = subset_sentiments[[subset_lowest_mean_index]]), 
                                  aes(x = Sentiment_Score)) +
  geom_density(fill = subset_lowest_mean_index, alpha = 0.5) +
  geom_vline(xintercept = subset_means[subset_lowest_mean_index], linetype = "dashed", color = "red") +
  geom_text(aes(x = subset_means[subset_lowest_mean_index], 
                label = sprintf("Mean: %.2f", subset_means[subset_lowest_mean_index])),
            y = 0, vjust = -1, color = "red", size = 3) +  # Add label for mean
  labs(title = paste("Subset", subset_lowest_mean_index, "Sentiment Score Density"), 
       x = "Sentiment Score", y = "Density") +
  theme_minimal()

# Print the plots
print(subset_highest_mean_plot)
print(subset_lowest_mean_plot)

```



```{r}
analysis %>%
  ggplot(aes(x=score)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(analysis$score), sd = sd(analysis$score)), color = "orange", size = 1) +  # Add smoothed curve
  ylab("Frequency/Density") +
  xlab("Sentiment Score") +
  ggtitle("Distribution of Sentiment Scores of the Tweets") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red")  # Add vertical line at score 0

```

```{r}
library(plotly)

# Convert 'date' variable to datetime format
tweets_df$date <- lubridate::as_datetime(tweets_df$date)

# Extract date and sentiment score
date_sentiment <- data.frame(date = tweets_df$date, sentiment = analysis$score)

# Plot sentiment over time (ggplot)
date_sentiment %>%
  ggplot(aes(x = date, y = sentiment)) +
  geom_line() +
  ylab("Sentiment Score") +
  xlab("Date") +
  ggtitle("Sentiment Over Time") +
  ggeasy::easy_center_title()

# Plot sentiment over time using Plotly
plot_ly(date_sentiment, x = ~date, y = ~sentiment, type = "scatter", mode = "lines") %>%
  layout(
    yaxis = list(title = "Sentiment Score"),
    xaxis = list(title = "Date"),
    title = "Sentiment Over Time"
  )
  
```


## Bootstrapping Pre-Regex
```{r bootstrap}
# Bootstrap with 100 resamplings
bootstrap_results <- replicate(100, analysis$score)
table(bootstrap_results)

# point estimate function
calculate_statistics <- function(data, indices) {
  bootstrap_sample <- data[indices]
  
  mean_value <- mean(bootstrap_sample)
  std_dev <- sd(bootstrap_sample)
  
  return(c(mean_value, std_dev))
}

set.seed(123) 
bootstrap_pt_estimates_data <- boot(data = bootstrap_results, statistic = calculate_statistics, R = 100)

# Confidence intervals
conf_intervals <- boot.ci(bootstrap_pt_estimates_data, type = "basic")  

# Means, standard deviations
means <- bootstrap_pt_estimates_data$t[, 1]
std_devs <- bootstrap_pt_estimates_data$t[, 2]


conf_intervals
#means
# std_devs

# Create a density plot for standard deviations
ggplot(data = NULL, aes(x = means)) +
  geom_density(fill = "red", color = "black") +
  labs(title = "Density Plot of Means", x = "Means") +
  theme_minimal()

# Create a density plot for standard deviations
ggplot(data = NULL, aes(x = std_devs)) +
  geom_density(fill = "red", color = "black") +
  labs(title = "Density Plot of Standard Deviations", x = "Standard Deviation Values") +
  theme_minimal()


# creating df to graphically visualize distribution of results
bootstrap_df <- data.frame(Sentiment_Score = c(bootstrap_results))

# Creating distribution for sentiment scores
ggplot(bootstrap_df, aes(x = Sentiment_Score)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sentiment Scores", x = "Sentiment Score", y = "Frequency")
```



```{r}
# Sample data (replace this with your actual data)
tweets_df <- data.frame(
  timestamp = seq(from = as.POSIXct("2022-04-09"), by = "hours", length.out = 100),
  sentiment = sample(c("positive", "negative"), size = 100, replace = TRUE)
)

# Create a time series plot using Plotly
plot <- tweets_df %>%
  plot_ly(x = ~timestamp, y = ~sentiment, type = 'scatter', mode = 'lines', color = ~sentiment) %>%
  layout(title = "Sentiment of Tweets Over Time",
         xaxis = list(title = "Timestamp"),
         yaxis = list(title = "Sentiment"))


```


```{r}
unique_day_month_dates <- unique(format(tweets_df$date, "%d, %b"))

# Print the list of unique day, month dates with tweets
cat("Unique day, month dates with tweets:", unique_day_month_dates, "\n")




```

