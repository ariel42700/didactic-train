---
title: "Sentiment Analysis"
output: html_document
date: "2023-12-01"
---
```{r}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sentiment Analysis on Tweets about Ronaldo

```{r}
# List of packages to install
packages_to_install <- c("hms", "lubridate", "tidytext", "tm", "wordcloud",
                         "igraph", "glue", "networkD3", "plyr", "stringr",
                         "ggplot2", "ggeasy", "plotly", "dplyr", "hms",
                         "lubridate", "magrittr", "tidyverse", "janeaustenr",
                         "widyr")

# Install packages
#chooseCRANmirror(graphics=FALSE)
#install.packages(packages_to_install)
library(hms)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(boot)

file_path <- "data/Cleaned_ronaldo_tweets.csv"

tweets_df <- read.csv(file_path)

str(tweets_df)

# load sentiment 
positive = scan('data/resources/positive-words.txt', what = 'character', comment.char = ';')

negative = scan('data/resources/negative-words.txt', what = 'character', comment.char = ';')
# add your list of words below as you wish if missing in above read lists
pos.words = c(positive,'upgrade','Congrats','prizes','prize','thanks','thnx',
              'Grt','gr8','plz','trending','recovering','brainstorm','leader')

neg.words = c(negative,'wtf','wait','waiting','epicfail','Fight','fighting',
              'arrest','no','not')
```

```{r Data Cleaning}

```


```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none') {
  require(plyr)
  require(stringr)
  
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}

chunk_size <- 100 # Adjust the size based on your preference

# Get the number of chunks
num_chunks <- ceiling(nrow(tweets_df) / chunk_size)

print(num_chunks)

```

```{r}
library(plotly)

cleanText <- tweets_df$content
analysis <- score.sentiment(cleanText, pos.words, neg.words)
table(analysis$score)


```

```{r}
# plot of sentiment frequencies
analysis %>%
  ggplot(aes(x=score)) +
  geom_histogram(binwidth = 1, fill = "lightblue", color = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(analysis$score), sd = sd(analysis$score)), color = "orange", size = 1) +  # Add smoothed curve
  ylab("Frequency/Density") +
  xlab("Sentiment Score") +
  ggtitle("Distribution of Sentiment Scores of the Tweets") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +  # Add vertical line at score 0
  ggeasy::easy_center_title()



```

```{r}
library(plotly)

# Convert 'date' variable to datetime format
tweets_df$date <- lubridate::as_datetime(tweets_df$date)

# Extract date and sentiment score
date_sentiment <- data.frame(date = tweets_df$date, sentiment = analysis$score)

# Plot sentiment over time (ggplot)
date_sentiment %>%
  ggplot(aes(x = date, y = sentiment)) +
  geom_line() +
  ylab("Sentiment Score") +
  xlab("Date") +
  ggtitle("Sentiment Over Time") +
  ggeasy::easy_center_title()

# Plot sentiment over time using Plotly
plot_ly(date_sentiment, x = ~date, y = ~sentiment, type = "scatter", mode = "lines") %>%
  layout(
    yaxis = list(title = "Sentiment Score"),
    xaxis = list(title = "Date"),
    title = "Sentiment Over Time"
  )
  
```


## Bootstrapping Pre-Regex
```{r bootstrap}
# Bootstrap with 100 resamplings
bootstrap_results <- replicate(100, analysis$score)
table(bootstrap_results)

# point estimate function
calculate_statistics <- function(data, indices) {
  bootstrap_sample <- data[indices]
  
  mean_value <- mean(bootstrap_sample)
  std_dev <- sd(bootstrap_sample)
  
  return(c(mean_value, std_dev))
}

set.seed(123) 
bootstrap_pt_estimates_data <- boot(data = bootstrap_results, statistic = calculate_statistics, R = 100)

# Confidence intervals
conf_intervals <- boot.ci(bootstrap_pt_estimates_data, type = "basic")  

# Means, standard deviations
means <- bootstrap_pt_estimates_data$t[, 1]
std_devs <- bootstrap_pt_estimates_data$t[, 2]


conf_intervals
#means
# std_devs

# Create a density plot for standard deviations
ggplot(data = NULL, aes(x = means)) +
  geom_density(fill = "red", color = "black") +
  labs(title = "Density Plot of Means", x = "Means") +
  theme_minimal()

# Create a density plot for standard deviations
ggplot(data = NULL, aes(x = std_devs)) +
  geom_density(fill = "red", color = "black") +
  labs(title = "Density Plot of Standard Deviations", x = "Standard Deviation Values") +
  theme_minimal()


# creating df to graphically visualize distribution of results
bootstrap_df <- data.frame(Sentiment_Score = c(bootstrap_results))

# Creating distribution for sentiment scores
ggplot(bootstrap_df, aes(x = Sentiment_Score)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sentiment Scores", x = "Sentiment Score", y = "Frequency")
```

## Regex 
```{r}
# Filter tweets containing the word "good" or "great" using regex
positive_tweets <- tweets_df[tweets_df$content %>% str_detect("(?i)\\b(good|great)\\b"), ]

# Extract context around "good" or "great" using regex
positive_tweets$context_around_good_great <- str_extract(positive_tweets$content, "(?i)\\b\\w+\\s+(good|great)\\s+\\w+\\b")

positive_tweets <- na.omit(positive_tweets)

# Display the positive tweets and the corresponding context
print(positive_tweets[, c("content", "context_around_good_great")])


```
```{r}
# Assign sentiment labels based on the presence of "not" or "no" in the context
positive_tweets$sentiment <- ifelse(str_detect(positive_tweets$context_around_good_great, "(?i)\\b(not|no)\\b"), "negative", "positive")

# Display the positive tweets, the corresponding context, and the assigned sentiment labels
print(positive_tweets[, c("content", "context_around_good_great", "sentiment")])
```

```{r}
# Count the number of tweets with negative sentiment
num_negative_tweets <- sum(positive_tweets$sentiment == "negative")


# Display the count of tweets with negative sentiment
cat("Number of tweets with negative sentiment:", num_negative_tweets, "\n")
```
```{r}
# Filter tweets containing the words "nasty," "disgusting," or "gross" using regex
target_words <- c("nasty", "disgusting", "gross", "filthy")
negative_tweets <- tweets_df[tweets_df$content %>% str_detect(paste0("(?i)\\b(", paste(target_words, collapse = "|"), ")\\b")), ]

# Extract context around the matched words
negative_tweets$context_around_words <- str_extract_all(negative_tweets$content, "(?i)(\\b\\w+\\s+\\w+\\s+\\b(?:nasty|disgusting|gross|filthy)\\b\\s+\\w+\\s+\\w+\\b)")

# Flatten the list of contexts
negative_tweets$context_around_words <- sapply(negative_tweets$context_around_words, function(x) paste(x, collapse = " | "))

# Display the negative tweets and the context around the matched words
print(negative_tweets[, c("content", "context_around_words")])

```
```{r}
# Sample dataset
text_data <- c(
  "This restaurant serves nasty food, but the ambiance is great.",
  "The movie was disgusting, but the cinematography was impressive.",
  "The streets were filthy after the heavy rain.",
  "The garbage bin smells gross; it needs to be emptied."
)

# Words to search for
target_words <- c("nasty", "gross", "filthy", "disgusting")

# Create a regex pattern for matching the target words
regex_pattern <- paste0("(?i)\\b(?:", paste(target_words, collapse = "|"), ")\\b")

# Extract context around the matched words
context_around_words <- str_extract_all(text_data, paste0("(.{0,20}\\b", regex_pattern, "\\b.{0,20})"))

# Flatten the list of contexts
context_around_words <- sapply(context_around_words, function(x) paste(x, collapse = " | "))

# Display the original text and the context around the matched words
result_df <- data.frame(original_text = text_data, context_around_words)
print(result_df)


```


```{r}
unique_day_month_dates <- unique(format(tweets_df$date, "%d, %b"))

# Print the list of unique day, month dates with tweets
cat("Unique day, month dates with tweets:", unique_day_month_dates, "\n")




```
=======

>>>>>>> 1274135ac8fe0c6b1ba4c892437b5d4e6be58972
